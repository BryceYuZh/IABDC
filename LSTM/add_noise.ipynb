{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Setting seed for reproducability\n",
    "np.random.seed(1234)  \n",
    "PYTHONHASHSEED = 0\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix, recall_score, precision_score\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, LSTM, Activation, Conv1D, MaxPooling1D, Masking, GaussianNoise\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./data/processed_train.csv')\n",
    "td = pd.read_csv('./data/processed_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.loc[train_df['RUL']>130,['RUL']]=130"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "        id  cycle  setting1  setting2  setting3        s1        s2        s3  \\\n0        1      1  0.833134  0.997625       1.0  0.060269  0.181576  0.311201   \n1        1      2  0.999767  0.998575       1.0  0.000000  0.131847  0.296600   \n2        1      3  0.595096  0.738480       0.0  0.238089  0.016332  0.035297   \n3        1      4  0.999993  0.999525       1.0  0.000000  0.128269  0.298795   \n4        1      5  0.595137  0.736698       0.0  0.238089  0.014130  0.037871   \n...    ...    ...       ...       ...       ...       ...       ...       ...   \n53754  260    312  0.476188  0.831354       1.0  0.626985  0.672172  0.682297   \n53755  260    313  0.238102  0.298100       1.0  0.597937  0.644830  0.733008   \n53756  260    314  0.595222  0.736342       0.0  0.238089  0.017892  0.088067   \n53757  260    315  0.595203  0.738717       0.0  0.238089  0.021195  0.079155   \n53758  260    316  0.833260  0.997625       1.0  0.060269  0.193687  0.354544   \n\n             s4        s5  ...       s20       s21  mode1  mode2  mode3  \\\n0      0.273095  0.146592  ...  0.156036  0.159082    0.0    0.0    0.0   \n1      0.245535  0.000000  ...  0.007888  0.014562    0.0    1.0    0.0   \n2      0.056997  0.293184  ...  0.133745  0.151414    0.0    0.0    1.0   \n3      0.246979  0.000000  ...  0.014060  0.026144    0.0    1.0    0.0   \n4      0.058152  0.293184  ...  0.135460  0.143240    0.0    0.0    1.0   \n...         ...       ...  ...       ...       ...    ...    ...    ...   \n53754  0.591489  0.507937  ...  0.486283  0.483993    0.0    0.0    0.0   \n53755  0.722934  0.617180  ...  0.614540  0.622022    1.0    0.0    0.0   \n53756  0.082198  0.293184  ...  0.137517  0.144474    0.0    0.0    1.0   \n53757  0.102368  0.293184  ...  0.132716  0.134383    0.0    0.0    1.0   \n53758  0.293049  0.146592  ...  0.156722  0.161215    0.0    0.0    0.0   \n\n       mode4  mode5  mode6  RUL  cycle_norm  \n0        1.0    0.0    0.0  130    0.000000  \n1        0.0    0.0    0.0  130    0.002653  \n2        0.0    0.0    0.0  130    0.005305  \n3        0.0    0.0    0.0  130    0.007958  \n4        0.0    0.0    0.0  130    0.010610  \n...      ...    ...    ...  ...         ...  \n53754    0.0    1.0    0.0    4    0.824934  \n53755    0.0    0.0    0.0    3    0.827586  \n53756    0.0    0.0    0.0    2    0.830239  \n53757    0.0    0.0    0.0    1    0.832891  \n53758    1.0    0.0    0.0    0    0.835544  \n\n[53759 rows x 34 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>cycle</th>\n      <th>setting1</th>\n      <th>setting2</th>\n      <th>setting3</th>\n      <th>s1</th>\n      <th>s2</th>\n      <th>s3</th>\n      <th>s4</th>\n      <th>s5</th>\n      <th>...</th>\n      <th>s20</th>\n      <th>s21</th>\n      <th>mode1</th>\n      <th>mode2</th>\n      <th>mode3</th>\n      <th>mode4</th>\n      <th>mode5</th>\n      <th>mode6</th>\n      <th>RUL</th>\n      <th>cycle_norm</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0.833134</td>\n      <td>0.997625</td>\n      <td>1.0</td>\n      <td>0.060269</td>\n      <td>0.181576</td>\n      <td>0.311201</td>\n      <td>0.273095</td>\n      <td>0.146592</td>\n      <td>...</td>\n      <td>0.156036</td>\n      <td>0.159082</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>130</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>2</td>\n      <td>0.999767</td>\n      <td>0.998575</td>\n      <td>1.0</td>\n      <td>0.000000</td>\n      <td>0.131847</td>\n      <td>0.296600</td>\n      <td>0.245535</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.007888</td>\n      <td>0.014562</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>130</td>\n      <td>0.002653</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>3</td>\n      <td>0.595096</td>\n      <td>0.738480</td>\n      <td>0.0</td>\n      <td>0.238089</td>\n      <td>0.016332</td>\n      <td>0.035297</td>\n      <td>0.056997</td>\n      <td>0.293184</td>\n      <td>...</td>\n      <td>0.133745</td>\n      <td>0.151414</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>130</td>\n      <td>0.005305</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>4</td>\n      <td>0.999993</td>\n      <td>0.999525</td>\n      <td>1.0</td>\n      <td>0.000000</td>\n      <td>0.128269</td>\n      <td>0.298795</td>\n      <td>0.246979</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.014060</td>\n      <td>0.026144</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>130</td>\n      <td>0.007958</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>5</td>\n      <td>0.595137</td>\n      <td>0.736698</td>\n      <td>0.0</td>\n      <td>0.238089</td>\n      <td>0.014130</td>\n      <td>0.037871</td>\n      <td>0.058152</td>\n      <td>0.293184</td>\n      <td>...</td>\n      <td>0.135460</td>\n      <td>0.143240</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>130</td>\n      <td>0.010610</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>53754</th>\n      <td>260</td>\n      <td>312</td>\n      <td>0.476188</td>\n      <td>0.831354</td>\n      <td>1.0</td>\n      <td>0.626985</td>\n      <td>0.672172</td>\n      <td>0.682297</td>\n      <td>0.591489</td>\n      <td>0.507937</td>\n      <td>...</td>\n      <td>0.486283</td>\n      <td>0.483993</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>4</td>\n      <td>0.824934</td>\n    </tr>\n    <tr>\n      <th>53755</th>\n      <td>260</td>\n      <td>313</td>\n      <td>0.238102</td>\n      <td>0.298100</td>\n      <td>1.0</td>\n      <td>0.597937</td>\n      <td>0.644830</td>\n      <td>0.733008</td>\n      <td>0.722934</td>\n      <td>0.617180</td>\n      <td>...</td>\n      <td>0.614540</td>\n      <td>0.622022</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3</td>\n      <td>0.827586</td>\n    </tr>\n    <tr>\n      <th>53756</th>\n      <td>260</td>\n      <td>314</td>\n      <td>0.595222</td>\n      <td>0.736342</td>\n      <td>0.0</td>\n      <td>0.238089</td>\n      <td>0.017892</td>\n      <td>0.088067</td>\n      <td>0.082198</td>\n      <td>0.293184</td>\n      <td>...</td>\n      <td>0.137517</td>\n      <td>0.144474</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2</td>\n      <td>0.830239</td>\n    </tr>\n    <tr>\n      <th>53757</th>\n      <td>260</td>\n      <td>315</td>\n      <td>0.595203</td>\n      <td>0.738717</td>\n      <td>0.0</td>\n      <td>0.238089</td>\n      <td>0.021195</td>\n      <td>0.079155</td>\n      <td>0.102368</td>\n      <td>0.293184</td>\n      <td>...</td>\n      <td>0.132716</td>\n      <td>0.134383</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>0.832891</td>\n    </tr>\n    <tr>\n      <th>53758</th>\n      <td>260</td>\n      <td>316</td>\n      <td>0.833260</td>\n      <td>0.997625</td>\n      <td>1.0</td>\n      <td>0.060269</td>\n      <td>0.193687</td>\n      <td>0.354544</td>\n      <td>0.293049</td>\n      <td>0.146592</td>\n      <td>...</td>\n      <td>0.156722</td>\n      <td>0.161215</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.835544</td>\n    </tr>\n  </tbody>\n</table>\n<p>53759 rows × 34 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add noice to it\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to reshape features into (samples, time steps, features) \n",
    "def gen_sequence(id_df, seq_length, seq_cols):\n",
    "    \"\"\" Only sequences that meet the window-length are considered, no padding is used. This means for testing\n",
    "    we need to drop those which are below the window-length. An alternative would be to pad sequences so that\n",
    "    we can use shorter ones \"\"\"\n",
    "    data_array = id_df[seq_cols].values\n",
    "    num_elements = data_array.shape[0]\n",
    "    for start, stop in zip(range(0, num_elements-seq_length), range(seq_length, num_elements)):\n",
    "        yield data_array[start:stop, :]\n",
    "    # For example id1 have 192 rows and sequence_length is equal to 50\n",
    "    # so zip iterate over two following list of numbers (0,112),(50,192)\n",
    "    # 0 50 -> from row 0 to row 50\n",
    "    # 1 51 -> from row 1 to row 51\n",
    "    # 2 52 -> from row 2 to row 52\n",
    "    # ...\n",
    "    # 111 191 -> from row 111 to 191"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick the feature columns \n",
    "sensor_cols = ['s' + str(i) for i in range(1,22)]\n",
    "sequence_cols = ['setting1', 'setting2', 'setting3','cycle_norm','mode1','mode2','mode3','mode4','mode5','mode6']\n",
    "# sequence_cols = ['setting1', 'setting2', 'setting3','cycle_norm']\n",
    "sequence_cols.extend(sensor_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(40759, 50, 31)"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "# generator for the sequences\n",
    "seq_gen = (list(gen_sequence(train_df[train_df['id']==id], sequence_length, sequence_cols)) \n",
    "           for id in train_df['id'].unique())\n",
    "\n",
    "# generate sequences and convert to numpy array\n",
    "seq_array = np.concatenate(list(seq_gen)).astype(np.float32)\n",
    "seq_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate labels\n",
    "def gen_labels(id_df, seq_length, label):\n",
    "    data_array = id_df[label].values\n",
    "    num_elements = data_array.shape[0]\n",
    "    return data_array[seq_length:num_elements, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate labels\n",
    "label_gen = [gen_labels(train_df[train_df['id']==id], sequence_length, ['RUL']) \n",
    "             for id in train_df['id'].unique()]\n",
    "label_array = np.concatenate(label_gen).astype(np.float32)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_keras(y_true, y_pred):\n",
    "    \"\"\"Coefficient of Determination \n",
    "    \"\"\"\n",
    "    SS_res =  K.sum(K.square( y_true - y_pred ))\n",
    "    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) )\n",
    "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoring_error(y_true, y_pred):\n",
    "    diff = y_pred - y_true\n",
    "    diff_pos = diff[diff>=0]\n",
    "    diff_neg = diff[diff<0]\n",
    "    score = K.sum(K.exp(-1./13.*diff_neg)-1.)+K.sum(K.exp(1./10.*diff_pos)-1.)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:From /Users/zhehaoyu/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n\nWARNING:tensorflow:From /Users/zhehaoyu/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n\nWARNING:tensorflow:From /Users/zhehaoyu/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n\nWARNING:tensorflow:From /Users/zhehaoyu/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4115: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n\nWARNING:tensorflow:From /Users/zhehaoyu/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n\nWARNING:tensorflow:From /Users/zhehaoyu/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2974: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.where in 2.0, which has the same broadcast rule as np.where\nWARNING:tensorflow:From /Users/zhehaoyu/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\nWARNING:tensorflow:From /Users/zhehaoyu/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nmasking_1 (Masking)          (None, 50, 31)            0         \n_________________________________________________________________\ngaussian_noise_1 (GaussianNo (None, 50, 31)            0         \n_________________________________________________________________\nlstm_1 (LSTM)                (None, 50, 100)           52800     \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 50, 100)           0         \n_________________________________________________________________\nlstm_2 (LSTM)                (None, 50)                30200     \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 50)                0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 1)                 51        \n_________________________________________________________________\nactivation_1 (Activation)    (None, 1)                 0         \n=================================================================\nTotal params: 83,051\nTrainable params: 83,051\nNon-trainable params: 0\n_________________________________________________________________\nNone\n"
    }
   ],
   "source": [
    "# Build model with mask\n",
    "nb_features = seq_array.shape[2]\n",
    "nb_out = label_array.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Masking(mask_value=0., input_shape=(sequence_length, nb_features)))\n",
    "model.add(GaussianNoise(0.1))\n",
    "model.add(LSTM(\n",
    "         units=100,\n",
    "         return_sequences=True,))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(\n",
    "          units=50,\n",
    "          return_sequences=False,))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=nb_out))\n",
    "model.add(Activation(\"linear\"))\n",
    "model.compile(loss='mean_squared_error', optimizer='rmsprop',metrics=['mae',r2_keras])\n",
    "# model.compile(loss=scoring_error, optimizer='rmsprop',metrics=['mae',r2_keras])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 38721 samples, validate on 2038 samples\nEpoch 1/100\n - 82s - loss: 5622.2617 - mean_absolute_error: 63.0922 - r2_keras: -2.1166e+00 - val_loss: 5826.4429 - val_mean_absolute_error: 64.5330 - val_r2_keras: -2.3025e+00\nEpoch 2/100\n - 74s - loss: 4688.0120 - mean_absolute_error: 56.9563 - r2_keras: -1.5944e+00 - val_loss: 4928.0742 - val_mean_absolute_error: 58.9412 - val_r2_keras: -1.7886e+00\nEpoch 3/100\n - 74s - loss: 3928.2320 - mean_absolute_error: 51.9269 - r2_keras: -1.1735e+00 - val_loss: 4152.1102 - val_mean_absolute_error: 54.0705 - val_r2_keras: -1.3461e+00\nEpoch 4/100\n - 74s - loss: 3287.2837 - mean_absolute_error: 47.6671 - r2_keras: -8.1834e-01 - val_loss: 3491.5413 - val_mean_absolute_error: 49.8814 - val_r2_keras: -9.7071e-01\nEpoch 5/100\n - 74s - loss: 2767.7518 - mean_absolute_error: 44.1787 - r2_keras: -5.2976e-01 - val_loss: 2948.7354 - val_mean_absolute_error: 46.3849 - val_r2_keras: -6.6394e-01\nEpoch 6/100\n - 75s - loss: 2369.3418 - mean_absolute_error: 41.4914 - r2_keras: -3.1029e-01 - val_loss: 2527.8681 - val_mean_absolute_error: 43.6111 - val_r2_keras: -4.2805e-01\nEpoch 7/100\n - 74s - loss: 2085.2165 - mean_absolute_error: 39.5203 - r2_keras: -1.5277e-01 - val_loss: 2224.0481 - val_mean_absolute_error: 41.5316 - val_r2_keras: -2.6017e-01\nEpoch 8/100\n - 74s - loss: 1917.2160 - mean_absolute_error: 38.3160 - r2_keras: -5.9040e-02 - val_loss: 2040.3240 - val_mean_absolute_error: 40.1819 - val_r2_keras: -1.6153e-01\nEpoch 9/100\n - 75s - loss: 1849.4608 - mean_absolute_error: 37.7235 - r2_keras: -2.0740e-02 - val_loss: 1975.8800 - val_mean_absolute_error: 39.6471 - val_r2_keras: -1.2886e-01\nEpoch 10/100\n - 74s - loss: 1847.6101 - mean_absolute_error: 37.7151 - r2_keras: -2.0615e-02 - val_loss: 2068.3547 - val_mean_absolute_error: 40.3056 - val_r2_keras: -1.7391e-01\nEpoch 11/100\n - 74s - loss: 1857.6399 - mean_absolute_error: 37.6043 - r2_keras: -2.5601e-02 - val_loss: 1939.5382 - val_mean_absolute_error: 39.2944 - val_r2_keras: -1.0940e-01\nEpoch 12/100\n - 74s - loss: 1557.6840 - mean_absolute_error: 33.8698 - r2_keras: 0.1398 - val_loss: 1578.0488 - val_mean_absolute_error: 35.3029 - val_r2_keras: 0.1001\nEpoch 13/100\n - 74s - loss: 1309.4533 - mean_absolute_error: 30.6435 - r2_keras: 0.2759 - val_loss: 1735.9347 - val_mean_absolute_error: 36.1263 - val_r2_keras: 0.0263\nEpoch 14/100\n - 74s - loss: 1189.5494 - mean_absolute_error: 28.7025 - r2_keras: 0.3417 - val_loss: 1637.3895 - val_mean_absolute_error: 34.4526 - val_r2_keras: 0.0825\nEpoch 15/100\n - 78s - loss: 1045.9982 - mean_absolute_error: 26.8800 - r2_keras: 0.4214 - val_loss: 1409.6973 - val_mean_absolute_error: 32.5225 - val_r2_keras: 0.2091\nEpoch 16/100\n - 75s - loss: 947.7390 - mean_absolute_error: 25.3816 - r2_keras: 0.4755 - val_loss: 2251.2780 - val_mean_absolute_error: 37.3485 - val_r2_keras: -2.7090e-01\nEpoch 17/100\n - 75s - loss: 885.6302 - mean_absolute_error: 24.3212 - r2_keras: 0.5102 - val_loss: 1232.2136 - val_mean_absolute_error: 29.3574 - val_r2_keras: 0.3062\nEpoch 18/100\n - 75s - loss: 838.9597 - mean_absolute_error: 23.5421 - r2_keras: 0.5360 - val_loss: 1060.3422 - val_mean_absolute_error: 28.0087 - val_r2_keras: 0.4026\nEpoch 19/100\n - 79s - loss: 816.6149 - mean_absolute_error: 23.0852 - r2_keras: 0.5484 - val_loss: 1262.1247 - val_mean_absolute_error: 29.4464 - val_r2_keras: 0.2950\nEpoch 20/100\n - 91s - loss: 797.1770 - mean_absolute_error: 22.7287 - r2_keras: 0.5579 - val_loss: 1157.5364 - val_mean_absolute_error: 27.8720 - val_r2_keras: 0.3524\nEpoch 21/100\n - 116s - loss: 776.9592 - mean_absolute_error: 22.3900 - r2_keras: 0.5699 - val_loss: 1108.2287 - val_mean_absolute_error: 27.4367 - val_r2_keras: 0.3791\nEpoch 22/100\n - 108s - loss: 756.4573 - mean_absolute_error: 22.0783 - r2_keras: 0.5813 - val_loss: 1127.1250 - val_mean_absolute_error: 28.0186 - val_r2_keras: 0.3695\nEpoch 23/100\n - 75s - loss: 741.0515 - mean_absolute_error: 21.8150 - r2_keras: 0.5898 - val_loss: 1542.8495 - val_mean_absolute_error: 30.9131 - val_r2_keras: 0.1426\nEpoch 24/100\n - 75s - loss: 720.8224 - mean_absolute_error: 21.4152 - r2_keras: 0.6004 - val_loss: 1000.4325 - val_mean_absolute_error: 26.4532 - val_r2_keras: 0.4326\nEpoch 25/100\n - 76s - loss: 692.0635 - mean_absolute_error: 20.9656 - r2_keras: 0.6167 - val_loss: 1183.1275 - val_mean_absolute_error: 27.3469 - val_r2_keras: 0.3242\nEpoch 26/100\n - 76s - loss: 670.0454 - mean_absolute_error: 20.5464 - r2_keras: 0.6288 - val_loss: 954.9789 - val_mean_absolute_error: 26.0082 - val_r2_keras: 0.4568\nEpoch 27/100\n - 76s - loss: 631.4512 - mean_absolute_error: 19.8985 - r2_keras: 0.6500 - val_loss: 924.2138 - val_mean_absolute_error: 25.7513 - val_r2_keras: 0.4703\nEpoch 28/100\n - 75s - loss: 597.8218 - mean_absolute_error: 19.2600 - r2_keras: 0.6690 - val_loss: 925.1092 - val_mean_absolute_error: 25.4814 - val_r2_keras: 0.4866\nEpoch 29/100\n - 75s - loss: 548.1125 - mean_absolute_error: 18.4875 - r2_keras: 0.6961 - val_loss: 1206.9063 - val_mean_absolute_error: 26.8962 - val_r2_keras: 0.3061\nEpoch 30/100\n - 74s - loss: 508.2469 - mean_absolute_error: 17.6871 - r2_keras: 0.7183 - val_loss: 1016.3032 - val_mean_absolute_error: 26.9679 - val_r2_keras: 0.4240\nEpoch 31/100\n - 74s - loss: 452.5403 - mean_absolute_error: 16.6258 - r2_keras: 0.7494 - val_loss: 1112.6581 - val_mean_absolute_error: 27.5298 - val_r2_keras: 0.3734\nEpoch 32/100\n - 75s - loss: 415.1507 - mean_absolute_error: 15.9173 - r2_keras: 0.7701 - val_loss: 1343.4035 - val_mean_absolute_error: 27.9876 - val_r2_keras: 0.2427\nEpoch 33/100\n - 75s - loss: 368.4396 - mean_absolute_error: 14.9056 - r2_keras: 0.7956 - val_loss: 1204.1882 - val_mean_absolute_error: 26.8947 - val_r2_keras: 0.3282\nEpoch 34/100\n - 74s - loss: 340.2906 - mean_absolute_error: 14.2857 - r2_keras: 0.8116 - val_loss: 1770.7520 - val_mean_absolute_error: 31.6246 - val_r2_keras: 0.0051\nEpoch 35/100\n - 74s - loss: 311.6633 - mean_absolute_error: 13.6435 - r2_keras: 0.8273 - val_loss: 1073.9558 - val_mean_absolute_error: 26.2682 - val_r2_keras: 0.3947\nEpoch 36/100\n - 75s - loss: 284.8928 - mean_absolute_error: 13.0380 - r2_keras: 0.8421 - val_loss: 1361.4357 - val_mean_absolute_error: 28.6174 - val_r2_keras: 0.2405\nEpoch 37/100\n - 74s - loss: 260.3099 - mean_absolute_error: 12.3956 - r2_keras: 0.8560 - val_loss: 1114.7913 - val_mean_absolute_error: 26.2425 - val_r2_keras: 0.3677\ndict_keys(['val_loss', 'val_mean_absolute_error', 'val_r2_keras', 'loss', 'mean_absolute_error', 'r2_keras'])\n"
    }
   ],
   "source": [
    "# fit the network\n",
    "model_path='deeplstm.h5'\n",
    "history = model.fit(seq_array, label_array, epochs=100, batch_size=256, validation_split=0.05, verbose=2,\n",
    "          callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='min'),\n",
    "                       keras.callbacks.ModelCheckpoint(model_path,monitor='val_loss', save_best_only=True, mode='min', verbose=0)]\n",
    "          )\n",
    "\n",
    "# list all data in history\n",
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "40759/40759 [==============================] - 37s 906us/step\nLoss: 241.46708622557622\n"
    }
   ],
   "source": [
    "scores = model.evaluate(seq_array, label_array, verbose=1, batch_size=200)\n",
    "print('Loss: {}'.format(scores[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = []\n",
    "for i in range(len(td.groupby('id'))):\n",
    "    if len(td[td['id']==(i+1)]) >= sequence_length:\n",
    "        test.append(np.asarray(td[td['id']==(i+1)][sequence_cols].values[-sequence_length:]).astype(np.float32).tolist())\n",
    "    else:\n",
    "        test.append(np.asarray(td[td['id']==(i+1)][sequence_cols].values[:]).astype(np.float32).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_inputs = keras.preprocessing.sequence.pad_sequences(test,dtype='float32',                                padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "259/259 [==============================] - 2s 7ms/step\n"
    }
   ],
   "source": [
    "result = model.predict(padded_inputs,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_excel('../results_submission.xls')\n",
    "submit['RUL prediction']=result\n",
    "submit.to_csv('deeplstm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(259,)"
     },
     "metadata": {},
     "execution_count": 82
    }
   ],
   "source": [
    "ans = np.loadtxt('../RUL_FD002.txt')\n",
    "ans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(user_ans):\n",
    "    user_ans_arr = np.array(user_ans)\n",
    "    score = 0\n",
    "    diff = user_ans_arr - ans\n",
    "    diff_pos = diff[diff>=0]\n",
    "    diff_neg = diff[diff<0]\n",
    "    score = np.sum((np.exp(-1./13.*diff_neg)-1))+np.sum((np.exp(1./10.*diff_pos)-1))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "re = pd.read_csv('deeplstm.csv')\n",
    "pred_rul=re['RUL prediction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "70475.54484426416"
     },
     "metadata": {},
     "execution_count": 85
    }
   ],
   "source": [
    "score(pred_rul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}